# =============================================================================
# PokéAgent Configuration Template
# =============================================================================
# Copy this file to .env, fill in the values you need, then run:
#   docker compose --env-file .env up
# =============================================================================

# -----------------------------------------------------------------------------
# Core Runtime Defaults
# -----------------------------------------------------------------------------
INSTANCE_NUMBER=0 # Set to a unique number for each instance
HOST_PORT=8000 # Set to a unique port for each instance

# AGENT_LOAD_STATE=tests/states/torchic.state
AGENT_LOAD_STATE=Emerald-GBAdvance/splits/00_init/00_init.state
# AGENT_LOAD_STATE=Emerald-GBAdvance/splits/01_tutorial/01_tutorial.state
# AGENT_LOAD_STATE=Emerald-GBAdvance/splits/02_starter/02_starter.state
# AGENT_LOAD_STATE=Emerald-GBAdvance/splits/03_birch/03_birch.state
# AGENT_LOAD_STATE=Emerald-GBAdvance/splits/04_rival/04_rival.state
# AGENT_LOAD_STATE=Emerald-GBAdvance/splits/05_petalburg/05_petalburg.state
# AGENT_LOAD_STATE=Emerald-GBAdvance/splits/06_road/06_road.state

# Set to true to keep existing stitched maps when loading new state files
AGENT_PRESERVE_MAP_CACHE=false
AGENT_ROM=Emerald-GBAdvance/rom.gba

# Toggle common run.py flags
RECORD_VIDEO=true
AGENT_AUTO=true
AGENT_NO_OCR=true
# Optional toggles:
# AGENT_HEADLESS=true
# AGENT_MANUAL=true

# Submission settings (organize recordings by langfuse_session_id)
# Recordings and submission.log will automatically be written to:
# SUBMISSION_DIR/langfuse_session_id/ during the run
SUBMISSION_DIR=submissions

# Action delay in seconds (time to wait after each agent action in AUTO mode)
# Lower values = faster agent execution, higher values = more time to observe actions
# Default: 0.5
AGENT_ACTION_DELAY=0.5

# -----------------------------------------------------------------------------
# Agent Selection
# -----------------------------------------------------------------------------
# Choose your agent using AGENT_MODE and AGENT_TYPE
# 
# LEGACY AGENTS (proven, production-ready):
#   AGENT_MODE=legacy, AGENT_TYPE=simple              # Fast, direct frame→action
#   AGENT_MODE=legacy, AGENT_TYPE=react               # ReAct thought loops
#   AGENT_MODE=legacy, AGENT_TYPE=explorer            # Explorer variant
#   AGENT_MODE=legacy, AGENT_TYPE=location_simple     # Location memory
#   AGENT_MODE=legacy, AGENT_TYPE=agent_1_explorer    # Exploration specialist
#   AGENT_MODE=legacy, AGENT_TYPE=agent_2_trainer     # Battle specialist
#   AGENT_MODE=legacy, AGENT_TYPE=agent_3_collector   # Item specialist
#   AGENT_MODE=legacy, AGENT_TYPE=agent_4_speedrunner # Speedrun specialist
#   AGENT_MODE=legacy, AGENT_TYPE=fourmodule          # Four-module architecture
#
# CUSTOM AGENTS (modular, experimental):
#   AGENT_MODE=custom, AGENT_TYPE=minimal             # Basic agent (no VLM)
#   AGENT_MODE=custom, AGENT_TYPE=minimal_vlm         # Basic VLM agent
#   AGENT_MODE=custom, AGENT_TYPE=minimal_vlm_langchain  # LangChain VLM agent
#   AGENT_MODE=custom, AGENT_TYPE=custom_vlm          # Extended VLM agent
#   AGENT_MODE=custom, AGENT_TYPE=navigation          # Navigation-focused agent
#   AGENT_MODE=custom, AGENT_TYPE=mvp_hierarchical   # MVP hierarchical agent
#   AGENT_MODE=custom, AGENT_TYPE=navigation-nt     # NT's navigation agent

AGENT_MODE=custom
AGENT_TYPE=mvp_hierarchical


# -----------------------------------------------------------------------------
# VLM Backend Selection
# -----------------------------------------------------------------------------
# Choose which backend run.py should prefer by default.
# AGENT_BACKEND=vllm
# AGENT_MODEL_NAME=Qwen/Qwen3-VL-30B-A3B-Instruct

AGENT_BACKEND=openai
AGENT_MODEL_NAME=gpt-5.1

# -----------------------------------------------------------------------------
# Azure Environment for Object Detection and Redis Caching (required for MVP Hierarchical Agent)
# -----------------------------------------------------------------------------
VISION_ENDPOINT=
VISION_KEY=
ENTRAID_CLIENT_ID=
ENTRAID_CLIENT_SECRET=
ENTRAID_TENANT_ID=
REDIS_HOST=

# -----------------------------------------------------------------------------
# VLM Backend Selection
# -----------------------------------------------------------------------------
# Choose which backend run.py should prefer by default.
# Choose from "openai", "openrouter", "vllm", "gemini", "vertex"
# AGENT_BACKEND=
# AGENT_MODEL_NAME=

# -----------------------------------------------------------------------------
# OpenAI Backend (GPT-4o, o3-mini, etc.)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-api-key-here

# -----------------------------------------------------------------------------
# OpenRouter Backend (Access to multiple model providers)
# -----------------------------------------------------------------------------
# Get your API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-your-openrouter-api-key-here

# -----------------------------------------------------------------------------
# vLLM / OpenAI-Compatible Backend (Self-hosted inference)
# -----------------------------------------------------------------------------
# Point to your vLLM server and optional API key.
# VLLM_BASE_URL=
# VLLM_API_KEY=

# Optional generation controls:
# VLLM_TEMPERATURE=0.7
# VLLM_MAX_TOKENS=1024
# VLLM_TOP_P=0.9

# -----------------------------------------------------------------------------
# Google Gemini Backend
# -----------------------------------------------------------------------------
# Get your API key from: https://aistudio.google.com/app/apikey
# Either variable name works:
# GEMINI_API_KEY=your-gemini-api-key-here
# GOOGLE_API_KEY=your-google-api-key-here

# -----------------------------------------------------------------------------
# Google Vertex AI Backend
# -----------------------------------------------------------------------------
# It's not really working yet, use gemini for now.

# -----------------------------------------------------------------------------
# Langfuse Tracing (Optional)
# -----------------------------------------------------------------------------
# Enable tracing of LLM calls for observability and debugging
# Get your credentials from your Langfuse instance
# LANGFUSE_ENABLED=true
# LANGFUSE_HOST=https://langfuse.shipit.systems
# LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
# LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here
# LANGFUSE_USER_ID=your-username-or-id
# Optional templated session identifier. Available tokens:
#   {AGENT_MODE} {AGENT_TYPE} {AGENT_MODEL_NAME} {AGENT_BACKEND} {DATETIME}
# LANGFUSE_SESSION_ID={AGENT_MODE}_{AGENT_TYPE}_{DATETIME}_{AGENT_MODEL_NAME}
